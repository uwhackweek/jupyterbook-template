{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-today",
   "metadata": {},
   "source": [
    "# GPR\n",
    "Lead Developer: Tate Meehan\n",
    "\n",
    "Co-developers: Dan McGrath & Ryan Webb\n",
    "\n",
    "**Overview**\n",
    "\n",
    "In this tutorial we will request the snow pit 1S1 location and density, ground-penetrating radar (GPR) two-way travel-times (TWT) and geolocation data, and Magnaprobe snow depths and locations to make a quick comparison of the Magnaprobe snow depth measurements and the GPR estimated snow depths.\n",
    "\n",
    "We will calculate the average density from the pit and visualize a set of GPR travel-times around Pit 1S1. Given the average dry snow density of 1S1, we will then use an empirically derived expression from Kovacs et. al (1995) to estimate the radar wave speed. The wave speed allows us to convert the radar two-way travel-time to snow depth.\n",
    "\n",
    "Lastly we will use a few summary statistics to compare the GPR and Magnaprobe snow depths, and we will discuss the various potential sources of error that arise naturally, systematically, and/or algorithmically.\n",
    "\n",
    "**Slides**\n",
    "\n",
    "https://docs.google.com/presentation/d/1Hh2CdCCvhWzcWzjzHi9WPum5NmOQt67B1vi1AwOzXCQ/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-webcam",
   "metadata": {},
   "source": [
    "## Retrieve density, GPR, and Magnaprobe data from Pit 1S1\n",
    "\n",
    "**Goal**: Compare the Magnaprobe snow depth to the GPR estimated snow depth from SnowEx 2020 Grand Mesa IOP Pit 1S1\n",
    "\n",
    " \n",
    "\n",
    "**Approach**: \n",
    "\n",
    "1. Retrieve the pit location from the Layer Data table \n",
    "2. Build a circle of 50 m radius around the pit location \n",
    "3. Request the pit data to get density layers and calculate the average\n",
    "4. Request all the GPR data within a 50 m distance of our pit\n",
    "5. Plot GPR TWT\n",
    "6. Convert TWT to depth using snow density\n",
    "7. Request the Magnaprobe depths around Pit 1S1\n",
    "8. Interpolate GPR depths to the locations of the Magnaprobe depths\n",
    "9. Compare statistics of GPR and Magnaprobe depths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-phoenix",
   "metadata": {},
   "source": [
    "## Process\n",
    "### Step 1: Get the pit/site coordinates\n",
    "We must first import the necessary libraries for operating with the SnowEx SQL database.\n",
    "We then import the Point Data (e.g. GPR) and Layer Data (e.g. snow pit) and GeoPandas, PostGIS, and Python functionality. We also establish the Pit Site ID (1S1) and the buffer radius around the pit (50 m)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import our DB access function\n",
    "from snowexsql.db import get_db\n",
    "\n",
    "# Import the two tables we need GPR ---> PointData, Density (Pits) --> LayerData \n",
    "from snowexsql.data import PointData, LayerData\n",
    "from snowexsql.conversions import query_to_geopandas\n",
    "\n",
    "# Import to make use of the postgis functions on the db that are not necessarily in python \n",
    "from sqlalchemy import func, Float\n",
    "# Import datetime module to filter by a date\n",
    "import datetime \n",
    "\n",
    "\n",
    "# use numpy to calculate the average of the density results \n",
    "import numpy as np \n",
    "\n",
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# PIT Site Identifier\n",
    "site_id = '1S1'\n",
    "\n",
    "# Distance around the pit to collect data in meters\n",
    "buffer_dist = 50\n",
    "\n",
    "# Connect to the database we made.\n",
    "db_name = 'snow:hackweek@db.snowexdata.org/snowex'\n",
    "engine, session = get_db(db_name)\n",
    "\n",
    "# Grab our pit geometry (position) object by provided site id from the site details table, Since there is multiple layers and dates we limit the request to 1\n",
    "q = session.query(LayerData.geom).filter(LayerData.site_id == site_id).limit(1)\n",
    "site = q.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-ocean",
   "metadata": {},
   "source": [
    "### Step 2: Build a buffered circle around our pit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the geometry point into text to be used by Postgis function ST_Buffer\n",
    "point = session.query(site[0].geom.ST_AsText()).all()\n",
    "\n",
    "print(point)\n",
    "\n",
    "# Create a polygon buffered by our distance centered on the pit using postgis ST_Buffer \n",
    "q = session.query(func.ST_Buffer(point[0][0], buffer_dist))\n",
    "buffered_pit = q.all()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-burner",
   "metadata": {},
   "source": [
    "### Step 3: Grab Density Profiles\n",
    "\n",
    "We query all Layer Data, cast these values to a float, and then compute the average. Then the query is filtered to only the data type 'density'. The output rho_avg_all is the average density of all snow pits, we also filter the query again by site_id to extract the average density of pit 1S1. These density values are then printed to the screen for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request the average (avg) of Layer data casted as a float. We have to cast to a float in the layer table because all main values are stored as a string to \n",
    "# ...accommodate the hand hardness. \n",
    "qry = session.query(func.avg(LayerData.value.cast(Float)))\n",
    "\n",
    "# Filter our query only to density \n",
    "qry = qry.filter(LayerData.type=='density')\n",
    "\n",
    "# Request the data \n",
    "rho_avg_all = qry.all()\n",
    "\n",
    "# Request the Average Density of Just 1S1\n",
    "rho_avg_1s1 = qry.filter(LayerData.site_id == site_id).limit(1)\n",
    "\n",
    "# This is a gotcha. The data in layer data only is stored as a string to accommodate the hand hardness values \n",
    "print(f\"Average density of all pits is {rho_avg_all[0][0]:0.0f} kg/m3\")\n",
    "print(f\"Average density of pit 1S1 is {rho_avg_1s1[0][0]:0.0f} kg/m3\")\n",
    "\n",
    "# Cast Densities to float\n",
    "rho_avg_all = float(rho_avg_all[0][0])\n",
    "rho_avg_1s1 = float(rho_avg_1s1[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-polymer",
   "metadata": {},
   "source": [
    "### Step 4: Request all GPR TWT measured inside the buffer\n",
    "In this step, we first print all of the instruments and data types contained in PointData. Doing so let's us know that in order to query the GPR two-way travel-times we use the identifier 'two_way_travel'. We then apply a filter for the date January 29, 2020, and refine this query further with the filter for TWT only within our buffered region. Using geopandas, the query is cast into a dataframe. By default the queried PointData type is given the name 'value'. To be more explicit we rename the variable as 'twt' within the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all Point Data where the instrument string contains the GPR in its name\n",
    "#qry = session.query(PointData).filter(PointData.instrument.contains('GPR'))\n",
    "# Print all types of PointData in the query\n",
    "tmp = session.query(PointData.instrument).distinct().all()\n",
    "print(tmp)\n",
    "# Print all types of PointData in the query\n",
    "tmp = session.query(PointData.type).distinct().all()\n",
    "print(tmp)\n",
    "\n",
    "qry = session.query(PointData).filter(PointData.type == 'two_way_travel')\n",
    "\n",
    "# Additionally Filter by a date \n",
    "qry = qry.filter(PointData.date==datetime.date(2020, 1, 29))\n",
    "\n",
    "# See upload details at https://github.com/SnowEx/snowexsql/blob/087b382b8d5098f09db67310efd49f777525c0c8/scripts/upload/add_gpr.py#L27\n",
    "\n",
    "# Grab all the point data in the buffer using the POSTGIS ST_Within, anytime using the postgis functions we typically have to convert to text\n",
    "qry = qry.filter(func.ST_Within(PointData.geom.ST_AsText(), buffered_pit.ST_AsText()))\n",
    "\n",
    "# Use our handy dandy function to execute the query and make it a geopandas dataframe\n",
    "dfGPR = query_to_geopandas(qry, engine)\n",
    "# rename 'value' in dataframe as 'twt'\n",
    "dfGPR.rename(columns={'value': 'twt'},inplace=True )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-singapore",
   "metadata": {},
   "source": [
    "### Step 5: Plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-dealing",
   "metadata": {
    "tags": [
     "nbsphinx-gallery",
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the Matplotlib Axes object from the dataframe object, color points by snow depth value\n",
    "ax = dfGPR.plot(column='twt', legend=True, cmap='PuBu')\n",
    "\n",
    "\n",
    "# Use non-scientific notation for x and y ticks\n",
    "ax.ticklabel_format(style='plain', useOffset=False)\n",
    "\n",
    "# Set the various plots x/y labels and title.\n",
    "ax.set_title('Grand Mesa GPR Travel-times w/in {}m of site {}'.format(buffer_dist, site_id))\n",
    "ax.set_xlabel('Easting [m]')\n",
    "ax.set_ylabel('Northing [m]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-parish",
   "metadata": {},
   "source": [
    "### Step 6: Convert TWT to Depth Using Snow Density\n",
    "We will relate the dry snow density to the electromagnetic wave speed using the Kovacs et. al (1995) formula\n",
    "\n",
    "$$\n",
    "\\epsilon_{\\mathrm{r}}^{\\prime}=(1+0.845 \\rho)^{2} \\quad .\n",
    "$$ (permitivity)\n",
    "\n",
    "Equation {eq}`permitivity` calculates the dielectric constant $\\epsilon_{\\mathrm{r}}^{\\prime}$, provided the snow density $\\rho$. We must then relate the dielectric constant to the electromagnetic wave speed $(v)$\n",
    "\n",
    "$$\n",
    "v = \\frac{c}{\\sqrt{\\epsilon_{\\mathrm{r}}^{\\prime}}} \\quad .\n",
    "$$ (wavespeed)\n",
    "In equation {eq}`wavespeed` $c$ is the universal constant $0.3~m/ns$.\n",
    "\n",
    "We then calculate the depth of the snow \n",
    "\n",
    "$$\n",
    "z = \\frac{vt}{2} \\quad ,\n",
    "$$ (depthconversion)\n",
    "\n",
    "using the two-way travel-time $(t)$ and the electromagnetic velocity.\n",
    "\n",
    "We add the GPR estimated snow depths to the dataframe, and print the head of the dataframe to confirm this addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Snow Density\n",
    "# all pits\n",
    "rho = rho_avg_all\n",
    "# 1s1\n",
    "rho = rho_avg_1s1\n",
    "# convert density to specific gravity\n",
    "rho = rho/1000\n",
    "# Calculate Dielectric Permittivity of Snow\n",
    "epsilon = (1+0.845*rho)**2\n",
    "c = 0.3 # m/ns\n",
    "v = c/np.sqrt(epsilon) # m/ns\n",
    "t = dfGPR.twt\n",
    "z = v*t/2*100\n",
    "# Add the GPR depths to the datafram\n",
    "dfGPR['depth'] = z\n",
    "dfGPR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-allocation",
   "metadata": {},
   "source": [
    "### Step 7: Get Depth Probes\n",
    "We can recall the PointData types from above in Step 4, and we choose 'depth' as the PointData type filter. Again to ensure we are only considering data that was acquired on the same day as the GPR, we filter the depth data by the date January 29, 2020. We further refine this search to the instrument type 'magnaprobe' and of those data only query the points within our buffer. Lastly, we send this query to a new dataframe using the geopandas functionality, and rename the 'value' column as 'depth'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by the dataset type depth\n",
    "qry = session.query(PointData).filter(PointData.type == 'depth')\n",
    "\n",
    "# Additionally Filter by a date \n",
    "qry = qry.filter(PointData.date==datetime.date(2020, 1, 29))\n",
    "\n",
    "# Additionally Filter by instrument  \n",
    "qry = qry.filter(PointData.instrument=='magnaprobe')\n",
    "\n",
    "# Grab all the point data in the buffer\n",
    "qry = qry.filter(func.ST_Within(PointData.geom.ST_AsText(), buffered_pit.ST_AsText()))\n",
    "\n",
    "# Execute the query\n",
    "# Use our handy dandy function to execute the query and make it a geopandas dataframe\n",
    "dfProbe = query_to_geopandas(qry, engine)\n",
    "\n",
    "# rename Probed Depths to dataframe\n",
    "dfProbe.rename(columns={'value': 'depth'},inplace=True )\n",
    "dfProbe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-alloy",
   "metadata": {},
   "source": [
    "### Step 8: Average GPR Depths to Compare with Probed Depths\n",
    "\n",
    "In this step we will compare the GPR estimated depths to the Magnaprobe depths. In order to accomplish this, we must interpolate the GPR locations to the locations of the probe. We will use inverse distance weighting as our interpolation method.\n",
    "Example Code taken from https://stackoverflow.com/questions/3104781/inverse-distance-weighted-idw-interpolation-with-python\n",
    "\n",
    "Inverse distance weighting is a weighted average interpolant. The weights are computed as the inverse of the distance between the GPR locations $(x,y)$ and the depth probe locations (the interpolated locations $(x_i,y_i)$) \n",
    "\n",
    "$$\n",
    "d = {\\sqrt{(x-x_i)^{2}+(y-y_i)^2}} \\quad ,  \\\\\n",
    "w = \\frac{1}{d} \\quad .\n",
    "$$ (weights)\n",
    "\n",
    "Equation {eq}`weights` is then normalized \n",
    "\n",
    "$$\n",
    "w = \\frac{w}{\\sum{w}} \\quad ,\n",
    "$$ (idweights)\n",
    "\n",
    "to sum to one. For the $i^{th}$ location these weights are multiplied by the GPR depths \n",
    "\n",
    "$$\n",
    "z_i = w_i*z \\quad ,\n",
    "$$ (applyweights)\n",
    "\n",
    "to compute a weighted average. \n",
    "\n",
    "In the following code implementation of the inverse distance weighting algorithm, the `subtract.outer` method of the universal functions (ufunc) within numpy is called which computes the distances in equation {eq}`weights` by looping through the interpolation points. The dot product (inner product) is then used to multiply the weights with the GPR depths. This algorithm relies on the use of a for loop within the `ufunc.outer` call, yet it seems quite efficient! A notable caveat of this algorithm, and a source of error, is that the interpolation considers all points in the domain, rather than a localized interpolation. An interpolation scheme that employs a search radius of three meters, rather than 50 meters (as established by the buffer distance in step one), would be preferable. \n",
    "\n",
    "We then assign the GPR estimated depths to the probe dataframe, and compute the error between the probed depths and the GPR depths. \n",
    "\n",
    "The head of the depth probe dataframe is printed to show that we have added the interpolated GPR depths and the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Distance Weighting Interpolation\n",
    "def simple_idw(x, y, z, xi, yi):\n",
    "    dist = distance_matrix(x,y, xi,yi)\n",
    "\n",
    "    # In IDW, weights are 1 / distance\n",
    "    weights = 1.0 / dist\n",
    "\n",
    "    # Make weights sum to one\n",
    "    weights /= weights.sum(axis=0)\n",
    "\n",
    "    # Multiply the weights for each interpolated point by all observed Z-values\n",
    "    zi = np.dot(weights.T, z)\n",
    "    return zi\n",
    "\n",
    "def distance_matrix(x0, y0, x1, y1):\n",
    "    obs = np.vstack((x0, y0)).T\n",
    "    interp = np.vstack((x1, y1)).T\n",
    "\n",
    "    # Make a distance matrix between pairwise observations\n",
    "    # Note: from <http://stackoverflow.com/questions/1871536>\n",
    "    # (Yay for ufuncs!)\n",
    "    d0 = np.subtract.outer(obs[:,0], interp[:,0])\n",
    "    d1 = np.subtract.outer(obs[:,1], interp[:,1])\n",
    "\n",
    "    return np.hypot(d0, d1)\n",
    "\n",
    "# Estimate the GPR Depths at the Probe Locations\n",
    "z = simple_idw(dfGPR.easting, dfGPR.northing, dfGPR.depth, dfProbe.easting, dfProbe.northing)\n",
    "# Assign the GPR depths to the Probe dataframe\n",
    "dfProbe['depthGPR'] = z\n",
    "# Calculate the Error\n",
    "err = dfProbe.depth-dfProbe.depthGPR\n",
    "# Assign the Error to the Probe Dataframe\n",
    "dfProbe['error'] = err\n",
    "dfProbe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-bristol",
   "metadata": {},
   "source": [
    "### Step 9. Plot the Depths, Correlation, and Errors\n",
    "\n",
    "In this final step, we will compare the GPR depths and probed depths. We compute the Pearson correlation\n",
    "\n",
    "$$\n",
    "r=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum\\left(x_{i}-\\bar{x}\\right)^{2} \\sum\\left(y_{i}-\\bar{y}\\right)^{2}}} \\quad ,\n",
    "$$ (pearson)\n",
    "\n",
    "where $x$ represents the probed depths and $y$ represents the GPR depths.\n",
    "\n",
    "We calculate the bias of the GPR estimated depths \n",
    "\n",
    "$$\n",
    "\\mathrm{ME}=\\frac{\\sum_{i=1}^{N} \\left( x_{i}-y_{i} \\right) }{N} \\quad ,\n",
    "$$ (me)\n",
    "\n",
    "as the mean error ($\\mathrm{ME}$). Our example at pit 1S1 is relatively unbiased with a $\\mathrm{ME}=1.3~cm$. This indicates that the sources of error are uncorrelated and that systematic errors are small. \n",
    "The root-mean-square error\n",
    "\n",
    "$$\n",
    "\\mathrm{RMSE}=\\sqrt{\\frac{\\sum_{i=1}^{N}\\left(x_{i}-y_{i}\\right)^{2}}{N}}\n",
    "$$ (rmse)\n",
    "\n",
    "is a measurement of the standard deviation of the errors, if we assume that the errors are normally distributed and independent. In this example the $\\mathrm{RMSE}=11~cm$, which is approximately $1/2$ of the L-band GPR wavelength.\n",
    "\n",
    "**Potential Sources of Error**\n",
    "1. Incorrect density used in depth conversion\n",
    "2. Depth probe entering the soil or air-gap beneath vegitation\n",
    "3. Geolocation errors\n",
    "4. Sample \"footprint\" size mismatch\n",
    "5. Interpolation\n",
    "\n",
    "Data biases can be caused by using the incorrect density value. A lower density value will bias the GPR estimated depths to larger values, whereas, a higher density value will bias the GPR depths to lower values. It has also been shown that the point of the probe can enter the soil, which biases the observed depths positively (McGrath et al., 2019). Similarly, vegitation beneath the snow cover can be a source of error. It is possible that the GPR signal is reflected from the air gap beneath snow that is not grounded. In this case the depth probe may contact the ground, though the GPR travel-time does not, leading to a positive bias. Co-location of the GPR and depth probe presents a third possible source of error. Inaccuracy of GPS measurements or probes not coincident with the GPR transect are likely sources of geolocation error. A fourth possible source of error in the comparison of these depths is the disagreement between the size of the GPR \"footprint\" (known as the fresnel zone radius), which is on the order of one meter, and the area of the probe tip which is about one centimeter. Because these instruments do not sample the same place on the ground, localized variability in the ground topography can lead to errors between the measured and estimated depths. As mentioned in the previous section, the choice of interpolation scheme will affect the accuracy of the co-located depths. It is important to understand the pros and cons of various interpolation algorithms and to document the choice of algorithm used and it's parameters. \n",
    "\n",
    "In the code ection we, first, display these summary statistics. Then we view this information graphically. The first plot is the scatter of the observed depths versus the estimated depths with the regression line. The second plot is a histogram of the errors (observed - estimated). The histogram shows a slight positive bias, which indicates that a combination of the errors discussed above resulted in the probe measuring snow depths $1~cm$ greater than the GPR on average. The final plot displays the errors in depth spatially. Roughly, by eye, it appears that areas with low travel-time ($\\sim4~ns$, southeast quadrant) overestimate the depth, perhaps due to smearing introduced by the non-localized inverse distance weighting algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Correlation\n",
    "r = np.corrcoef(dfProbe.depth,dfProbe.depthGPR)\n",
    "print('The correlation is', round(r[0,1],2))\n",
    "# Calculate the Mean Error\n",
    "bias = np.mean(dfProbe.error)\n",
    "print('The bias is', round(bias,2), 'cm')\n",
    "# Calculate the Mean Absolute Error\n",
    "mae = np.mean(np.abs(dfProbe.error))\n",
    "# Calculate the Root Mean Squared Error\n",
    "rmse = np.sqrt(np.mean((dfProbe.error)**2))\n",
    "print('The rmse is', round(rmse,2), 'cm')\n",
    "# Compute the Regression Line\n",
    "m, b = np. polyfit(dfProbe.depth,dfProbe.depthGPR, 1)\n",
    "\n",
    "# Plot the Correlation\n",
    "plt.figure(0)\n",
    "plt.plot(dfProbe.depth,dfProbe.depthGPR,'o')\n",
    "plt.plot(dfProbe.depth, m*dfProbe.depth + b,'k')\n",
    "plt.xlabel('Probe Depth [cm]')\n",
    "plt.ylabel('GPR Depth [cm]')\n",
    "\n",
    "# Plot a Histogram of the Errors\n",
    "plt.figure(1)\n",
    "plt.hist(dfProbe.error, density=True, bins=10, edgecolor='black')  # density=False would make counts\n",
    "plt.ylabel('PDF')\n",
    "plt.xlabel('Error [cm]');\n",
    "# Get the Matplotlib Axes object from the dataframe object, color points by snow depth value\n",
    "ax = dfProbe.plot(column='error', legend=True, cmap='PuBu')\n",
    "\n",
    "\n",
    "# Use non-scientific notation for x and y ticks\n",
    "ax.ticklabel_format(style='plain', useOffset=False)\n",
    "\n",
    "# Set the various plots x/y labels and title.\n",
    "ax.set_title('Error [cm] (Probed Depth - GPR Depth)')\n",
    "ax.set_xlabel('Easting [m]')\n",
    "ax.set_ylabel('Northing [m]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the session to avoid hanging transactions\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}