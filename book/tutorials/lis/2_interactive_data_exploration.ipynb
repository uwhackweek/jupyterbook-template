{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liked-split",
   "metadata": {},
   "source": [
    "# Interactive Data Exploration\n",
    "\n",
    "This notebook demonstrates how the functions and techniques we covered in the first notebook can be combined to build interactive data exploration tools. The code in the cells below will generate two interactive panels. The first panel enables comparison of LIS output, SNODAS, and SNOTEL snow depth and snow water equivalent at SNOTEL site locations. The second panel enables exploration of LIS output using an interactive map.\n",
    "\n",
    "**Note: some cells below take several minutes to run and some aspects of the interactive widgets may not work in the rendered version on the Hackweek site.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-plastic",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import s3fs\n",
    "from datetime import datetime as dt\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import holoviews as hv, geoviews as gv\n",
    "from geoviews import opts\n",
    "from geoviews import tile_sources as gvts\n",
    "\n",
    "from datashader.colors import viridis\n",
    "import datashader\n",
    "from holoviews.operation.datashader import datashade, shade, dynspread, spread, rasterize\n",
    "\n",
    "from holoviews.streams import Selection1D, Params\n",
    "import panel as pn\n",
    "import param as pm\n",
    "import hvplot.pandas \n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create S3 filesystem object\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "# define S3 bucket name\n",
    "bucket = \"s3://eis-dh-hydro/SNOWEX-HACKWEEK\"\n",
    "\n",
    "# set holoviews backend to Bokeh\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-kentucky",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-blackberry",
   "metadata": {},
   "source": [
    "### SNOTEL Sites info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary linking state names and abbreviations\n",
    "snotel = {\"AZ\" : \"arizona\",\n",
    "          \"CO\" : \"colorado\",\n",
    "          \"ID\" : \"idaho\",\n",
    "          \"MT\" : \"montana\", \n",
    "          \"NM\" : \"newmexico\",\n",
    "          \"UT\" : \"utah\",\n",
    "          \"WY\" : \"wyoming\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-decade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNOTEL site metadata for sites in the given state\n",
    "def load_site(state):\n",
    "    \n",
    "    # define path to file\n",
    "    key = f\"SNOTEL/snotel_{state}.csv\"\n",
    "    \n",
    "    # load csv into pandas DataFrame\n",
    "    df = pd.read_csv(s3.open(f'{bucket}/{key}', mode='r'))\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-material",
   "metadata": {},
   "source": [
    "### SNOTEL Depth & SWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_snotel_txt(state, var):\n",
    "    \n",
    "    # define path to file\n",
    "    key = f\"SNOTEL/snotel_{state}{var}_20162020.txt\"\n",
    "\n",
    "    # open text file\n",
    "    fh = s3.open(f\"{bucket}/{key}\")\n",
    "    \n",
    "    # read each line and note those that begin with '#'\n",
    "    lines = fh.readlines()\n",
    "    skips = sum(1 for ln in lines if ln.decode('ascii').startswith('#'))\n",
    "    \n",
    "    # load txt file into pandas DataFrame (skipping lines beginning with '#')\n",
    "    df = pd.read_csv(s3.open(f\"{bucket}/{key}\"), skiprows=skips)\n",
    "    \n",
    "    # convert Date column from str to pandas datetime objects\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNOTEL depth & swe into dictionaries\n",
    "\n",
    "# define empty dicts\n",
    "snotel_depth = {}\n",
    "snotel_swe = {}\n",
    "\n",
    "# loop over states and load SNOTEL data\n",
    "for state in snotel.keys():\n",
    "    print(f\"Loading state {state}\")\n",
    "    snotel_depth[state] = load_snotel_txt(state, 'depth')\n",
    "    snotel_swe[state] = load_snotel_txt(state, 'swe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-pregnancy",
   "metadata": {},
   "source": [
    "### SNODAS Depth & SWE\n",
    "\n",
    "Like the LIS output we have been working with, a sample of SNODAS data is available on our S3 bucket in Zarr format. We can therefore load the SNODAS just as we load the LIS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load snodas depth data\n",
    "key = \"SNODAS/snodas_snowdepth_20161001_20200930.zarr\"\n",
    "snodas_depth = xr.open_zarr(s3.get_mapper(f\"{bucket}/{key}\"), consolidated=True)\n",
    "\n",
    "# load snodas swe data\n",
    "key = \"SNODAS/snodas_swe_20161001_20200930.zarr\"\n",
    "snodas_swe = xr.open_zarr(s3.get_mapper(f\"{bucket}/{key}\"), consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-preference",
   "metadata": {},
   "source": [
    "### LIS Outputs\n",
    "\n",
    "Next we'll load the LIS outputs. First, we'll define the helper function we saw in the previous notebook that adds `lat` and `lon` as coordinate variables. We'll use this immediately upon loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_latlon_coords(dataset: xr.Dataset)->xr.Dataset:\n",
    "    \"\"\"Adds lat/lon as dimensions and coordinates to an xarray.Dataset object.\"\"\"\n",
    "    \n",
    "    # get attributes from dataset\n",
    "    attrs = dataset.attrs\n",
    "    \n",
    "    # get x, y resolutions\n",
    "    dx = round(float(attrs['DX']), 3)\n",
    "    dy = round(float(attrs['DY']), 3)\n",
    "    \n",
    "    # get grid cells in x, y dimensions\n",
    "    ew_len = len(dataset['east_west'])\n",
    "    ns_len = len(dataset['north_south'])\n",
    "    \n",
    "    # get lower-left lat and lon\n",
    "    ll_lat = round(float(attrs['SOUTH_WEST_CORNER_LAT']), 3)\n",
    "    ll_lon = round(float(attrs['SOUTH_WEST_CORNER_LON']), 3)\n",
    "    \n",
    "    # calculate upper-right lat and lon\n",
    "    ur_lat =  ll_lat + (dy * ns_len)\n",
    "    ur_lon = ll_lon + (dx * ew_len)\n",
    "    \n",
    "    # define the new coordinates\n",
    "    coords = {\n",
    "        # create an arrays containing the lat/lon at each gridcell\n",
    "        'lat': np.linspace(ll_lat, ur_lat, ns_len, dtype=np.float32, endpoint=False),\n",
    "        'lon': np.linspace(ll_lon, ur_lon, ew_len, dtype=np.float32, endpoint=False)\n",
    "    }\n",
    "    \n",
    "   \n",
    "    # drop the original lat and lon variables\n",
    "    dataset = dataset.rename({'lon': 'orig_lon', 'lat': 'orig_lat'})\n",
    "    # rename the grid dimensions to lat and lon\n",
    "    dataset = dataset.rename({'north_south': 'lat', 'east_west': 'lon'})\n",
    "    # assign the coords above as coordinates\n",
    "    dataset = dataset.assign_coords(coords)\n",
    "    # reassign variable attributes\n",
    "    dataset.lon.attrs = dataset.orig_lon.attrs\n",
    "    dataset.lat.attrs = dataset.orig_lat.attrs\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-period",
   "metadata": {},
   "source": [
    "Load the LIS data and apply `add_latlon_coords()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIS surfacemodel DA_10km\n",
    "key = \"DA_SNODAS/SURFACEMODEL/LIS_HIST.d01.zarr\"\n",
    "\n",
    "lis_sf = xr.open_zarr(s3.get_mapper(f\"{bucket}/{key}\"), consolidated=True)\n",
    "\n",
    "# (optional for 10km simulation?)\n",
    "lis_sf = add_latlon_coords(lis_sf)\n",
    "\n",
    "# drop off irrelevant variables\n",
    "drop_vars = ['_history', '_eis_source_path', 'orig_lat', 'orig_lon']\n",
    "lis_sf = lis_sf.drop(drop_vars)\n",
    "lis_sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-campaign",
   "metadata": {},
   "source": [
    "Working with the full LIS output dataset can be slow and consume lots of memory. Here we temporally subset the data to a shorter window of time. The full dataset contains daily values from 10/1/2016 to 9/30/2018. Feel free to explore the full dataset by modifying the `time_range` variable below and re-running all cells that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset LIS data for two years  \n",
    "time_range = slice('2016-10-01', '2017-04-30')\n",
    "lis_sf = lis_sf.sel(time=time_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-intermediate",
   "metadata": {},
   "source": [
    "In the next cell, we extract the data variable names and timesteps from the LIS outputs. These will be used to define the widget options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather metadata from LIS\n",
    "\n",
    "# get variable names:string\n",
    "vnames = list(lis_sf.data_vars)\n",
    "print(vnames)\n",
    "\n",
    "# get time-stamps:string\n",
    "tstamps = list(np.datetime_as_string(lis_sf.time.values, 'D'))\n",
    "print(len(tstamps), tstamps[0], tstamps[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-floor",
   "metadata": {},
   "source": [
    "By default, the `holoviews` plotting library automatically adjusts the range of plot colorbars based on the range of values in the data being plotted. This may not be ideal when comparing data on different timesteps. In the next cell we extract the upper and lower bounds for each data variable which we'll later use to set a static colorbar range.\n",
    "\n",
    "**Note: this cell will take ~1m40s to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-load min/max range for LIS variables\n",
    "def get_cmap_range(vns):\n",
    "    vals = [(lis_sf[x].sel(time='2016-12').min(skipna=True).values.item(),\n",
    "        lis_sf[x].sel(time='2016-12').max(skipna=True).values.item()) for x in vns]\n",
    "    return dict(zip(vns, vals))\n",
    "\n",
    "cmap_lims = get_cmap_range(vnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-estimate",
   "metadata": {},
   "source": [
    "## Interactive Widgets\n",
    "\n",
    "### SNOTEL Site Map and Timeseries\n",
    "\n",
    "The two cells that follow will create an interactive panel for comparing LIS, SNODAS, and SNOTEL snow depth and snow water equivalent. The SNOTEL site locations are plotted as points on an interactive map for each state. Hover over the sites to view metadata and click on a site to generate a timeseries!\n",
    "\n",
    "**Note: it will take some time for the timeseries to display.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-asbestos",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get snotel depth\n",
    "def get_depth(state, site, ts, te):\n",
    "    df = snotel_depth[state]\n",
    "    \n",
    "    # subset between time range\n",
    "    mask = (df['Date'] >= ts) & (df['Date'] <= te)\n",
    "    df = df.loc[mask]\n",
    "    \n",
    "    # extract timeseries for the site\n",
    "    return pd.concat([df.Date, df.filter(like=site)], axis=1).set_index('Date')\n",
    "\n",
    "# get snotel swe\n",
    "def get_swe(state, site, ts, te):\n",
    "    df = snotel_swe[state]\n",
    "    \n",
    "    # subset between time range\n",
    "    mask = (df['Date'] >= ts) & (df['Date'] <= te)\n",
    "    df = df.loc[mask]\n",
    "    \n",
    "    # extract timeseries for the site\n",
    "    return pd.concat([df.Date, df.filter(like=site)], axis=1).set_index('Date')\n",
    "\n",
    "# co-locate site & LIS model cell\n",
    "def nearest_grid(pt):\n",
    "    # pt : input point, tuple (longtitude, latitude)\n",
    "    # output:\n",
    "    #        x_idx, y_idx \n",
    "    loc_valid = df_loc.dropna()\n",
    "    pts = loc_valid[['lon', 'lat']].to_numpy()\n",
    "    idx = distance.cdist([pt], pts).argmin()\n",
    "\n",
    "    return loc_valid['east_west'].iloc[idx], loc_valid['north_south'].iloc[idx]\n",
    "\n",
    "# get LIS variable \n",
    "def var_subset(dset, v, lon, lat, ts, te):\n",
    "    return dset[v].sel(lon=lon, lat=lat, method=\"nearest\").sel(time=slice(ts, te)).load()\n",
    "\n",
    "# line plots\n",
    "def line_callback(index, state, vname, ts_tag, te_tag):\n",
    "    sites = load_site(snotel[state])\n",
    "    row = sites.iloc[0]\n",
    "    \n",
    "    tmp = var_subset(lis_sf, vname, row.lon, row.lat, ts_tag, te_tag) \n",
    "    xr_sf = xr.zeros_like(tmp)\n",
    "    \n",
    "    xr_snodas = xr_sf\n",
    "    \n",
    "    ck = get_depth(state, row.site_name, ts_tag, te_tag).to_xarray().rename({'Date': 'time'})\n",
    "    xr_snotel = xr.zeros_like(ck)\n",
    "    \n",
    "    if not index:\n",
    "        title='Var: -- Lon: -- Lat: --'\n",
    "        return (xr_sf.hvplot(title=title, color='blue', label='LIS') \\\n",
    "                * xr_snotel.hvplot(color='red', label='SNOTEL') \\\n",
    "                * xr_snodas.hvplot(color='green', label='SNODAS')).opts(legend_position='right')\n",
    "        \n",
    "\n",
    "    else:\n",
    "        sites = load_site(snotel[state])\n",
    "        first_index = index[0]\n",
    "        row = sites.iloc[first_index]\n",
    "    \n",
    "        \n",
    "        xr_sf = var_subset(lis_sf, vname, row.lon, row.lat, ts_tag, te_tag)\n",
    "    \n",
    "        vs = vname.split('_')[0]\n",
    "        title=f'Var: {vs} Lon: {row.lon} Lat: {row.lat}'\n",
    "\n",
    "        \n",
    "        # update snotel data \n",
    "        if 'depth' in vname.lower():\n",
    "            xr_snotel =  get_depth(state, row.site_name, ts_tag, te_tag).to_xarray().rename({'Date': 'time'})*0.01\n",
    "            xr_snodas =  var_subset(snodas_depth, 'SNOWDEPTH', row.lon, row.lat, ts_tag, te_tag)*0.001\n",
    "        \n",
    "        if 'swe' in vname.lower():\n",
    "            xr_snotel =  get_swe(state, row.site_name, ts_tag, te_tag).to_xarray().rename({'Date': 'time'})\n",
    "            xr_snodas =  var_subset(snodas_swe, 'SWE', row.lon, row.lat, ts_tag, te_tag)\n",
    "\n",
    "    \n",
    "        return xr_sf.hvplot(title=title, color='blue', label='LIS') \\\n",
    "               * xr_snotel.hvplot(color='red', label='SNOTEL') \\\n",
    "               * xr_snodas.hvplot(color='green', label='SNODAS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-model",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sites on map\n",
    "def plot_points(state):  \n",
    "    # dataframe to hvplot obj Points\n",
    "    sites=load_site(snotel[state])\n",
    "    pts_opts=dict(size=12, nonselection_alpha=0.4,tools=['tap', 'hover'])\n",
    "    site_points=sites.hvplot.points(x='lon', y='lat', c='elev', cmap='fire', geo=True, hover_cols=['site_name', 'ntwk', 'state', 'lon', 'lat']).opts(**pts_opts)  \n",
    "    return site_points\n",
    "\n",
    "# base map\n",
    "tiles = gvts.OSM()\n",
    "\n",
    "# state widget\n",
    "state_select = pn.widgets.Select(options=list(snotel.keys()), name=\"State\")\n",
    "state_stream = Params(state_select, ['value'], rename={'value':'state'})\n",
    "\n",
    "# variable widget\n",
    "var_select = pn.widgets.Select(options=['SnowDepth_tavg', 'SWE_tavg'], name=\"LIS Variable List\")\n",
    "var_stream = Params(var_select, ['value'], rename={'value':'vname'})\n",
    "\n",
    "# date range widget\n",
    "date_fmt = '%Y-%m-%d'\n",
    "sdate_input = pn.widgets.DatetimeInput(name='Start date', value=dt(2016,10,1),start=dt.strptime(tstamps[0], date_fmt), end=dt.strptime(tstamps[-1], date_fmt), format=date_fmt)\n",
    "sdate_stream = Params(sdate_input, ['value'], rename={'value':'ts_tag'})\n",
    "edate_input = pn.widgets.DatetimeInput(name='End date', value=dt(2017,3,31),start=dt.strptime(tstamps[0], date_fmt), end=dt.strptime(tstamps[-1], date_fmt),format=date_fmt)\n",
    "edate_stream = Params(edate_input, ['value'], rename={'value':'te_tag'})\n",
    "\n",
    "# generate site points as dynamic map\n",
    "# plots points and calls plot_points() when user selects a site\n",
    "site_dmap = hv.DynamicMap(plot_points, streams=[state_stream]).opts(height=400, width=600)\n",
    "# pick site\n",
    "select_stream = Selection1D(source=site_dmap)\n",
    "\n",
    "# link widgets to callback function\n",
    "line = hv.DynamicMap(line_callback, streams=[select_stream, state_stream, var_stream, sdate_stream, edate_stream])\n",
    "\n",
    "# create panel layout\n",
    "pn.Row(site_dmap*tiles, pn.Column(state_select, var_select, pn.Row(sdate_input, edate_input), line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-soundtrack",
   "metadata": {},
   "source": [
    "### Interactive LIS Output Explorer\n",
    "\n",
    "The cell below creates a `panel` layout for exploring LIS output rasters. Select a variable using the drop down and then use the date slider to scrub back and forth in time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date widget (slider & key in)\n",
    "# start and end dates\n",
    "date_fmt = '%Y-%m-%d'\n",
    "b = dt.strptime('2016-10-01', date_fmt)\n",
    "e = dt.strptime('2017-04-30', date_fmt)\n",
    "\n",
    "# define date widgets\n",
    "date_slider = pn.widgets.DateSlider(start=b, end=e, value=b, name=\"LIS Model Date\")\n",
    "dt_input = pn.widgets.DatetimeInput(name='LIS Model Date Input', value=b, format=date_fmt)\n",
    "date_stream = Params(date_slider, ['value'], rename={'value':'date'})\n",
    "\n",
    "# variable widget\n",
    "var_select = pn.widgets.Select(options=vnames, name=\"LIS Variable List\")\n",
    "var_stream = Params(var_select, ['value'], rename={'value':'vname'})\n",
    "\n",
    "\n",
    "# base map widget\n",
    "map_layer= pn.widgets.RadioButtonGroup(\n",
    "    name='Base map layer',\n",
    "    options=['Open Street Map', 'Satellite Imagery'],\n",
    "    value='Satellite Imagery',\n",
    "    button_type='primary',\n",
    "    background='#f307eb')\n",
    "\n",
    "# lis output display callback function\n",
    "# returns plot of LIS output when date/variable is changed\n",
    "def var_layer(vname, date):\n",
    "    t_stamp = dt.strftime(date, '%Y-%m-%d')\n",
    "    dssm = lis_sf[vname].sel(time=t_stamp)\n",
    "\n",
    "    image = dssm.hvplot(geo=True)\n",
    "    clim = cmap_lims[vname]\n",
    "    return image.opts(clim=clim)\n",
    "\n",
    "# watches date widget for updates\n",
    "@pn.depends(dt_input.param.value, watch=True)\n",
    "def _update_date(dt_input):\n",
    "    date_slider.value=dt_input\n",
    "\n",
    "# updates basemap on widget change\n",
    "def update_map(maps):\n",
    "    tile = gvts.OSM if maps=='Open Street Map' else gvts.EsriImagery\n",
    "    return tile.opts(alpha=0.7)\n",
    "\n",
    "# link widgets to callback functions\n",
    "streams = dict(vname=var_select.param.value, date=date_slider.param.value)        \n",
    "dmap = hv.DynamicMap(var_layer, streams=streams)\n",
    "dtile = hv.DynamicMap(update_map, streams=dict(maps=map_layer.param.value))\n",
    "\n",
    "# create panel layout of widgets and plot\n",
    "pn.Column(var_select, date_slider, dt_input, map_layer,\n",
    "          dtile*rasterize(dmap, aggregator=datashader.mean()).opts(cmap=viridis,colorbar=True,width=800, height=600))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-teens",
   "metadata": {},
   "source": [
    "## Fin\n",
    "\n",
    "Thank you for joining us for this tutorial. We hope that you are now more familiar with [NASA's Land Information System](https://lis.gsfc.nasa.gov/) and how to use Python to explore and use the model simulation output LIS generates. For more information please see the links under the \"More information\" dropdown on the introduction page of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-algorithm",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
