{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outside-class",
   "metadata": {},
   "source": [
    "# Visualizing and Comparing LIS Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-somerset",
   "metadata": {},
   "source": [
    "```{figure} ./images/nasa-lis-combined-logos2.png\n",
    "---\n",
    "width: 300px\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-psychiatry",
   "metadata": {},
   "source": [
    "## LIS Output Primer\n",
    "\n",
    "LIS writes model state variables to disk at a frequency selected by the user (e.g., 6-hourly, daily, monthly). The LIS output we will be exploring was originally generated as *daily* NetCDF files, meaning one NetCDF was written per simulated day. We have converted these NetCDF files into a [Zarr](https://zarr.readthedocs.io/en/stable/) store for improved performance in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-clinton",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface to Amazon S3 filesystem\n",
    "import s3fs\n",
    "\n",
    "# interact with n-d arrays\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# interact with tabular data (incl. spatial)\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# interactive plots\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "\n",
    "# used to find nearest grid cell to a given location\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# set bokeh as the holoviews plotting backend\n",
    "hv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-grenada",
   "metadata": {},
   "source": [
    "## Load the LIS Output\n",
    "\n",
    "The `xarray` library makes working with labelled n-dimensional arrays easy and efficient. If you're familiar with the `pandas` library it should feel pretty familiar.\n",
    "\n",
    "Here we load the LIS output into an `xarray.Dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create S3 filesystem object\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# define the name of our S3 bucket\n",
    "bucket_name = 'eis-dh-hydro/SNOWEX-HACKWEEK'\n",
    "\n",
    "# define path to store on S3\n",
    "lis_output_s3_path = f's3://{bucket_name}/DA_SNODAS/SURFACEMODEL/LIS_HIST.d01.zarr/'\n",
    "\n",
    "# create key-value mapper for S3 object (required to read data stored on S3)\n",
    "lis_output_mapper = s3.get_mapper(lis_output_s3_path)\n",
    "\n",
    "# open the dataset\n",
    "lis_output_ds = xr.open_zarr(lis_output_mapper, consolidated=True)\n",
    "\n",
    "# drop some unneeded variables\n",
    "lis_output_ds = lis_output_ds.drop_vars(['_history', '_eis_source_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-ultimate",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "Display an interactive widget for inspecting the dataset by running a cell containing the variable name. Expand the dropdown menus and click on the document and database icons to inspect the variables and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-battlefield",
   "metadata": {},
   "source": [
    "### Accessing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-safety",
   "metadata": {},
   "source": [
    "Dataset attributes (metadata) are accessible via the `attrs` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-consultancy",
   "metadata": {},
   "source": [
    "### Accessing Variables\n",
    "\n",
    "Variables can be accessed using either **dot notation** or **square bracket notation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot notation\n",
    "lis_output_ds.SnowDepth_tavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square bracket notation\n",
    "lis_output_ds['SnowDepth_tavg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-massage",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Which syntax should I use?\n",
    "While both syntaxes perform the same function, the square-bracket syntax is useful when interacting with a dataset programmatically. For example, we can define a variable `varname` that stores the name of the variable in the dataset we want to access and then use that with the square-brackets notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'SnowDepth_tavg'\n",
    "\n",
    "lis_output_ds[varname]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-modern",
   "metadata": {},
   "source": [
    "The dot notation syntax will not work this way because `xarray` tries to find a variable in the dataset named `varname` instead of the value of the `varname` variable. When `xarray` can't find this variable, it throws an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run the code below to see the error\n",
    "\n",
    "# varname = 'SnowDepth_tavg'\n",
    "\n",
    "# lis_output_ds.varname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-right",
   "metadata": {},
   "source": [
    "### Dimensions and Coordinate Variables\n",
    "\n",
    "The dimensions and coordinate variable fields put the \"*labelled*\" in \"labelled n-dimensional arrays\":\n",
    "\n",
    "* **Dimensions:** labels for each dimension in the dataset (e.g., `time`)\n",
    "* **Coordinates:** labels for indexing along dimensions (e.g., `'2019-01-01'`)\n",
    "\n",
    "We can use these labels to select, slice, and aggregate the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-banking",
   "metadata": {},
   "source": [
    "#### Selecting/Subsetting\n",
    "\n",
    "`xarray` provides two methods for selecting or subsetting along coordinate variables:\n",
    "\n",
    "* index selection: `ds.isel(time=0)`\n",
    "* value selection `ds.sel(time='2019-01-01')`\n",
    "\n",
    "For example, we can select the first timestep from our dataset using index selection by passing the dimension name as a keyword argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember: python indexes start at 0\n",
    "lis_output_ds.isel(time=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-antarctica",
   "metadata": {},
   "source": [
    "Or we can use value selection to select based on the coordinate(s) (think \"labels\") of a given dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(time='2018-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-accordance",
   "metadata": {},
   "source": [
    "The `.sel()` approach also allows the use of shortcuts in some cases. For example, here we select all timesteps in the month of January 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(time='2018-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-baseline",
   "metadata": {},
   "source": [
    "Select a custom range of dates using Python's built-in `slice()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(time=slice('2018-01-01', '2018-01-15'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-radius",
   "metadata": {},
   "source": [
    "#### Latitude and Longitude\n",
    "\n",
    "You may have noticed that latitude (`lat`) and longitude (`lon`) are listed as data variables, not coordinate variables. This dataset would be easier to work with if `lat` and `lon` were coordinate variables and dimensions. Here we define a helper function that reads the spatial information from the dataset attributes, generates arrays containing the `lat` and `lon` values, and appends them to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_latlon_coords(dataset: xr.Dataset)->xr.Dataset:\n",
    "    \"\"\"Adds lat/lon as dimensions and coordinates to an xarray.Dataset object.\"\"\"\n",
    "    \n",
    "    # get attributes from dataset\n",
    "    attrs = dataset.attrs\n",
    "    \n",
    "    # get x, y resolutions\n",
    "    dx = round(float(attrs['DX']), 3)\n",
    "    dy = round(float(attrs['DY']), 3)\n",
    "    \n",
    "    # get grid cells in x, y dimensions\n",
    "    ew_len = len(dataset['east_west'])\n",
    "    ns_len = len(dataset['north_south'])\n",
    "    \n",
    "    # get lower-left lat and lon\n",
    "    ll_lat = round(float(attrs['SOUTH_WEST_CORNER_LAT']), 3)\n",
    "    ll_lon = round(float(attrs['SOUTH_WEST_CORNER_LON']), 3)\n",
    "    \n",
    "    # calculate upper-right lat and lon\n",
    "    ur_lat =  ll_lat + (dy * ns_len)\n",
    "    ur_lon = ll_lon + (dx * ew_len)\n",
    "    \n",
    "    # define the new coordinates\n",
    "    coords = {\n",
    "        # create an arrays containing the lat/lon at each gridcell\n",
    "        'lat': np.linspace(ll_lat, ur_lat, ns_len, dtype=np.float32, endpoint=False),\n",
    "        'lon': np.linspace(ll_lon, ur_lon, ew_len, dtype=np.float32, endpoint=False)\n",
    "    }\n",
    "    \n",
    "    lon_attrs = dataset.lon.attrs\n",
    "    lat_attrs = dataset.lat.attrs\n",
    "    \n",
    "    # rename the original lat and lon variables\n",
    "    dataset = dataset.rename({'lon':'orig_lon', 'lat':'orig_lat'})\n",
    "    # rename the grid dimensions to lat and lon\n",
    "    dataset = dataset.rename({'north_south': 'lat', 'east_west': 'lon'})\n",
    "    # assign the coords above as coordinates\n",
    "    dataset = dataset.assign_coords(coords)\n",
    "    dataset.lon.attrs = lon_attrs\n",
    "    dataset.lat.attrs = lat_attrs\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-upgrade",
   "metadata": {},
   "source": [
    "Now that the function is defined, let's use it to append `lat` and `lon` coordinates to the LIS output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds = add_latlon_coords(lis_output_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-wages",
   "metadata": {},
   "source": [
    "Inspect the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-street",
   "metadata": {},
   "source": [
    "Now `lat` and `lon` are listed as coordinate variables and have replaced the `north_south` and `east_west` dimensions. This will make it easier to spatially subset the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-partnership",
   "metadata": {},
   "source": [
    "#### Basic Spatial Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-writing",
   "metadata": {},
   "source": [
    "We can use the `slice()` function we used above on the `lat` and `lon` dimensions to select data between a range of latitudes and longitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(lat=slice(37, 41), lon=slice(-110, -101))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-green",
   "metadata": {},
   "source": [
    "Notice how the sizes of the `lat` and `lon` dimensions have decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-robin",
   "metadata": {},
   "source": [
    "#### Subset Across Multiple Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-folks",
   "metadata": {},
   "source": [
    "Select snow depth for Jan 2017 within a range of lat/lon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a range of dates to select\n",
    "wy_2018_slice = slice('2017-10-01', '2018-09-30')\n",
    "lat_slice = slice(37, 41)\n",
    "lon_slice = slice(-109, -102)\n",
    "\n",
    "# select the snow depth and subset to wy_2018_slice\n",
    "snd_CO_wy2018_ds = lis_output_ds['SnowDepth_tavg'].sel(time=wy_2018_slice, lat=lat_slice, lon=lon_slice)\n",
    "\n",
    "# inspect resulting dataset\n",
    "snd_CO_wy2018_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-third",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "We've imported two plotting libraries:\n",
    "\n",
    "* `matplotlib`: static plots\n",
    "* `hvplot`: interactive plots\n",
    "\n",
    "We can make a quick `matplotlib`-based plot for the subsetted data using the `.plot()` function supplied by `xarray.Dataset` objects. For this example, we'll select one day and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple matplotlilb plot\n",
    "snd_CO_wy2018_ds.sel(time='2018-01-01').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-pattern",
   "metadata": {},
   "source": [
    "Similarly we can make an interactive plot using the `hvplot` accessor and specifying a `quadmesh` plot type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hvplot based map\n",
    "snd_CO_20180101_plot = snd_CO_wy2018_ds.sel(time='2018-01-01').hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                                                                               xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                                                                               tiles='EsriImagery')\n",
    "\n",
    "snd_CO_20180101_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-monday",
   "metadata": {},
   "source": [
    "Pan, zoom, and scroll around the map. Hover over the LIS data to see the data values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-snapshot",
   "metadata": {},
   "source": [
    "If we try to plot more than one time-step `hvplot` will also provide a time-slider we can use to scrub back and forth in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "snd_CO_wy2018_ds.sel(time='2018-01').hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                             xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                             tiles='EsriImagery')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-output",
   "metadata": {},
   "source": [
    "From here on out we will stick with `hvplot` for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-centre",
   "metadata": {},
   "source": [
    "#### Timeseries Plots\n",
    "\n",
    "We can generate a timeseries for a given grid cell by selecting and calling the plot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define point to take timeseries (note: must be present in coordinates of dataset)\n",
    "ts_lon, ts_lat = (-105.65, 40.35)\n",
    "\n",
    "# plot timeseries (hvplot knows how to plot based on dataset's dimensionality!)\n",
    "snd_CO_wy2018_ds.sel(lat=ts_lat, lon=ts_lon).hvplot(title=f'Snow Depth Timeseries @ Lon: {ts_lon}, Lat: {ts_lat}',\n",
    "                                                   xlabel='Date', ylabel='Snow Depth (m)') + \\\n",
    "    snd_CO_20180101_plot * gv.Points([(ts_lon, ts_lat)]).opts(size=10, color='red')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-portuguese",
   "metadata": {},
   "source": [
    "In the next section we'll learn how to create a timeseries over a broader area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-wages",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "We can perform aggregation operations on the dataset such as `min()`, `max()`, `mean()`, and `sum()` by specifying the dimensions along which to perform the calculation.\n",
    "\n",
    "For example we can calculate the mean and maximum snow depth at each grid cell over water year 2018 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean at each grid cell over the time dimension\n",
    "mean_snd_CO_wy2018_ds = snd_CO_wy2018_ds.mean(dim='time')\n",
    "max_snd_CO_wy2018_ds = snd_CO_wy2018_ds.max(dim='time')\n",
    "\n",
    "# plot the mean and max snow depth\n",
    "mean_snd_CO_wy2018_ds.hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                                   xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                                   tiles='EsriImagery', title='Mean Snow Depth - WY2018') + \\\n",
    "    max_snd_CO_wy2018_ds.hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                                   xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                                   tiles='EsriImagery', title='Max Snow Depth - WY2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-chart",
   "metadata": {},
   "source": [
    "### Area Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-fields",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take area-averaged mean at each timestep\n",
    "mean_snd_CO_wy2018_ds = snd_CO_wy2018_ds.mean(['lat', 'lon'])\n",
    "\n",
    "# inspect the dataset\n",
    "mean_snd_CO_wy2018_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot timeseries (hvplot knows how to plot based on dataset's dimensionality!)\n",
    "mean_snd_CO_wy2018_ds.hvplot(title='Mean LIS Snow Depth for Colorado', xlabel='Date', ylabel='Snow Depth (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-kentucky",
   "metadata": {},
   "source": [
    "## Comparing LIS Output\n",
    "\n",
    "Now that we're familiar with the LIS output, let's compare it to two other datasets: SNODAS (raster) and SNOTEL (point)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-going",
   "metadata": {},
   "source": [
    "### LIS (raster) vs. SNODAS (raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-vermont",
   "metadata": {},
   "source": [
    "First, we'll load the SNODAS dataset which we also have hosted on S3 as a Zarr store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNODAS dataset\n",
    "\n",
    "#snodas depth\n",
    "key = \"SNODAS/snodas_snowdepth_20161001_20200930.zarr\"    \n",
    "snodas_depth_ds = xr.open_zarr(s3.get_mapper(f\"{bucket_name}/{key}\"), consolidated=True)\n",
    "\n",
    "# apply scale factor to convert to meters (0.001 per SNODAS user guide)\n",
    "snodas_depth_ds = snodas_depth_ds * 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-awareness",
   "metadata": {},
   "source": [
    "Next we define a helper function to extract the (lon, lat) of the nearest grid cell to a given point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_grid(ds, pt):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the nearest lon and lat to pt in a given Dataset (ds).\n",
    "    \n",
    "    pt : input point, tuple (longitude, latitude)\n",
    "    output:\n",
    "        lon, lat\n",
    "    \"\"\"\n",
    "    \n",
    "    if all(coord in list(ds.coords) for coord in ['lat', 'lon']):\n",
    "        df_loc = ds[['lon', 'lat']].to_dataframe().reset_index()\n",
    "    else:\n",
    "        df_loc = ds[['orig_lon', 'orig_lat']].isel(time=0).to_dataframe().reset_index()\n",
    "    \n",
    "    loc_valid = df_loc.dropna()\n",
    "    pts = loc_valid[['lon', 'lat']].to_numpy()\n",
    "    idx = distance.cdist([pt], pts).argmin()\n",
    "    \n",
    "    return loc_valid['lon'].iloc[idx], loc_valid['lat'].iloc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-vacation",
   "metadata": {},
   "source": [
    "The next cell will look pretty similar to what we did earlier to plot a timeseries of a single point in the LIS data. The general steps are:\n",
    "\n",
    "* Extract the coordinates of the SNODAS grid cell nearest to our LIS grid cell (`ts_lon` and `ts_lat` from earlier)\n",
    "* Subset the SNODAS and LIS data to the grid cells and date ranges of interest\n",
    "* Create the plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lon, lat of snodas grid cell nearest to the LIS coordinates we used earlier\n",
    "snodas_ts_lon, snodas_ts_lat = nearest_grid(snodas_depth_ds, (ts_lon, ts_lat))\n",
    "\n",
    "# define a date range to plot (shorter = quicker for demo)\n",
    "start_date, end_date = ('2018-01-01', '2018-03-01')\n",
    "plot_daterange = slice(start_date, end_date)\n",
    "\n",
    "# select SNODAS grid cell and subset to plot_daterange\n",
    "snodas_snd_subset_ds = snodas_depth_ds.sel(lon=snodas_ts_lon,\n",
    "                                             lat=snodas_ts_lat,\n",
    "                                             time=plot_daterange)\n",
    "\n",
    "# select LIS grid cell and subset to plot_daterange\n",
    "lis_snd_subset_ds = lis_output_ds['SnowDepth_tavg'].sel(lat=ts_lat,\n",
    "                                                        lon=ts_lon,\n",
    "                                                        time=plot_daterange)\n",
    "\n",
    "# create SNODAS snow depth plot\n",
    "snodas_snd_plot = snodas_snd_subset_ds.hvplot(label='SNODAS')\n",
    "\n",
    "# create LIS snow depth plot\n",
    "lis_snd_plot = lis_snd_subset_ds.hvplot(label='LIS')\n",
    "\n",
    "# create SNODAS vs LIS snow depth plot\n",
    "lis_vs_snodas_snd_plot = (lis_snd_plot * snodas_snd_plot)\n",
    "\n",
    "# display the plot\n",
    "lis_vs_snodas_snd_plot.opts(title=f'Snow Depth @ Lon: {ts_lon}, Lat: {ts_lat}',\n",
    "                            legend_position='right',\n",
    "                            xlabel='Date',\n",
    "                            ylabel='Snow Depth (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-medication",
   "metadata": {},
   "source": [
    "### LIS (raster) vs. SNODAS (raster) vs. SNOTEL (point)\n",
    "\n",
    "Now let's add SNOTEL point data to our plot.\n",
    "\n",
    "First, we're going to define some helper functions to load the SNOTEL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv containing metadata for SNOTEL sites in a given state (e.g,. 'colorado')\n",
    "def load_site(state):\n",
    "    \n",
    "    # define the path to the file\n",
    "    key = f\"SNOTEL/snotel_{state}.csv\"\n",
    "    \n",
    "    # load the csv into a pandas DataFrame\n",
    "    df = pd.read_csv(s3.open(f's3://{bucket_name}/{key}', mode='r'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# load SNOTEL data for a specific site\n",
    "def load_snotel_txt(state, var):\n",
    "    \n",
    "    # define the path to the file\n",
    "    key = f\"SNOTEL/snotel_{state}{var}_20162020.txt\"\n",
    "    \n",
    "    # determine how many lines to skip in the file (they start with #)\n",
    "    fh = s3.open(f\"{bucket_name}/{key}\")\n",
    "    lines = fh.readlines()\n",
    "    skips = sum(1 for ln in lines if ln.decode('ascii').startswith('#'))\n",
    "    \n",
    "    # load the data into a pandas DataFrame\n",
    "    df = pd.read_csv(s3.open(f\"s3://{bucket_name}/{key}\"), skiprows=skips)\n",
    "    \n",
    "    # convert the Date column from strings to datetime objects\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-person",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial let's load the SNOTEL data for sites in Colorado. We'll pick one site to plot in a few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNOTEL snow depth for Colorado into a dictionary\n",
    "snotel_depth = {'CO': load_snotel_txt('CO', 'depth')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-hotel",
   "metadata": {},
   "source": [
    "We'll need another helper function to load the depth data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get snotel depth\n",
    "def get_depth(state, site, start_date, end_date):\n",
    "    \n",
    "    # grab the depth for the given state (e.g., CO)\n",
    "    df = snotel_depth[state]\n",
    "    \n",
    "    # define a date range mask\n",
    "    mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n",
    "    \n",
    "    # use mask to subset between time range\n",
    "    df = df.loc[mask]\n",
    "    \n",
    "    # extract timeseries for the given site\n",
    "    return pd.concat([df.Date, df.filter(like=site)], axis=1).set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-donor",
   "metadata": {},
   "source": [
    "Load the site metadata for Colorado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_sites = load_site('colorado')\n",
    "\n",
    "# peek at the first 5 rows\n",
    "co_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-swiss",
   "metadata": {},
   "source": [
    "The point we've been using so far in the tutorial actually corresponds to the coordinates for the Bear Lake SNOTEL site along the Front Range! Let's extract the site data for that point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the depth data by passing the site name to the get_depth() function\n",
    "bear_lake_snd_df = get_depth('CO', 'Bear Lake (322)', start_date, end_date)\n",
    "\n",
    "# convert from cm to m\n",
    "bear_lake_snd_df = bear_lake_snd_df / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-richmond",
   "metadata": {},
   "source": [
    "Now we're ready to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SNOTEL plot\n",
    "bear_lake_plot = bear_lake_snd_df.hvplot(label='SNOTEL')\n",
    "\n",
    "# combine the SNOTEl plot with the LIS vs SNODAS plot\n",
    "(bear_lake_plot * lis_vs_snodas_snd_plot).opts(title=f'Snow Depth @ Lon: {ts_lon}, Lat: {ts_lat}', legend_position='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-hepatitis",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "You should now be more familiar with LIS data and how to interact with it in Python. The code in this notebook is a great jumping off point for developing more advanced comparisons and interactive widgets. For an example of what is possible, open the next notebook and run all the cells (Run > Run All Cells). After a few minutes, two interactive widgets will appear that allow you to explore and compare LIS output with SNODAS and SNOTEL data.\n",
    "\n",
    "The Python code can be adapted to other LIS simulations and to other model output as well, with minor modifications. Anyone interested in testing your new skills can combine what you learned here with the other SnowEx Hackweek tutorials - try comparing the LIS output with other snow observations collected during the 2017 field campaign!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-motivation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
